{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4289678,"sourceType":"datasetVersion","datasetId":2527538}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/diabetes-dataset'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-21T22:02:39.376165Z","iopub.execute_input":"2024-03-21T22:02:39.376525Z","iopub.status.idle":"2024-03-21T22:02:39.388711Z","shell.execute_reply.started":"2024-03-21T22:02:39.376499Z","shell.execute_reply":"2024-03-21T22:02:39.387662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/diabetes-dataset/diabetes.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:02:40.929459Z","iopub.execute_input":"2024-03-21T22:02:40.929840Z","iopub.status.idle":"2024-03-21T22:02:40.949605Z","shell.execute_reply.started":"2024-03-21T22:02:40.929812Z","shell.execute_reply":"2024-03-21T22:02:40.948869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:02:42.863750Z","iopub.execute_input":"2024-03-21T22:02:42.864318Z","iopub.status.idle":"2024-03-21T22:02:42.891354Z","shell.execute_reply.started":"2024-03-21T22:02:42.864283Z","shell.execute_reply":"2024-03-21T22:02:42.890417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T19:04:49.206300Z","iopub.execute_input":"2024-03-18T19:04:49.206794Z","iopub.status.idle":"2024-03-18T19:04:49.223405Z","shell.execute_reply.started":"2024-03-18T19:04:49.206751Z","shell.execute_reply":"2024-03-18T19:04:49.221919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport pandas as pd\n\nfrom sklearn.linear_model import LogisticRegression\ntarget_column='Outcome'\nX_df = df.drop(target_column, axis=1)\ny_df = df[target_column]\nX_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n# Split the dataset into features and target variable\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\n\n\n# Initialize the models\nmodels = {\n    'RandomForestClassifier': RandomForestClassifier(),\n    'LogisticRegression': LogisticRegression(),\n    'SVC': SVC(),\n    'DecisionTreeClassifier': DecisionTreeClassifier(),\n    'GaussianNB': GaussianNB(),\n    'KNeighborsClassifier': KNeighborsClassifier()\n}\n\n# Fit the models and print the accuracy\nmodel_performance_df = {}\n\nfor model_name, model in models.items():\n    # Train the model on the training set\n    model.fit(X_train_df, y_train_df)\n    \n    # Predict on the test set\n    predictions = model.predict(X_test_df)\n    \n    # Calculate the accuracy\n    accuracy = accuracy_score(y_test_df, predictions)\n    \n    # Store the performance\n    model_performance_df[model_name] = accuracy\n\n# Display the performance of each model on df_poly test set\nfor model_name, accuracy in model_performance_df.items():\n    print(f\"The accuracy of {model_name} on df test set is: {accuracy:.2f}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:02:47.115959Z","iopub.execute_input":"2024-03-21T22:02:47.116296Z","iopub.status.idle":"2024-03-21T22:02:48.002297Z","shell.execute_reply.started":"2024-03-21T22:02:47.116268Z","shell.execute_reply":"2024-03-21T22:02:48.001181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Assuming 'models' is a dictionary with your trained models\n# And 'accuracy_score' has been used to calculate accuracies\n\n# Dictionary to hold model names and their corresponding accuracies\nmodel_accuracies = {}\n\n# Calculate accuracies for each model and store them in the dictionary\nfor name, model in models.items():\n    y_pred_df = model.predict(X_test_df)\n    model_accuracies[name] = accuracy_score(y_test_df, y_pred_df)\n\n# Data for plotting\nmodel_names = list(model_accuracies.keys())\naccuracies = list(model_accuracies.values())\n\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Plotting the bar chart\nplt.figure(figsize=(10, 6))\nsns.barplot(x=model_names, y=accuracies, palette='viridis')\n\n# Adding the accuracy values on top of the bars\nfor i in range(len(model_names)):\n    plt.text(i, accuracies[i] + 0.01, f\"{accuracies[i]:.2f}\", ha = 'center')\n\nplt.xlabel('Model', fontsize=14)\nplt.ylabel('Accuracy', fontsize=14)\nplt.title('Comparison of Model Accuracies', fontsize=16)\nplt.ylim([0, 1])  # Accuracy ranges from 0 to 1\nplt.xticks(rotation=45, fontsize=12)\nplt.yticks(fontsize=12)\nplt.tight_layout()  # Adjusts plot to ensure everything fits without overlapping\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:03:43.853806Z","iopub.execute_input":"2024-03-21T22:03:43.854154Z","iopub.status.idle":"2024-03-21T22:03:44.274726Z","shell.execute_reply.started":"2024-03-21T22:03:43.854131Z","shell.execute_reply":"2024-03-21T22:03:44.273746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# Assuming 'df' is your dataframe and 'models' is a dictionary with your trained models\n\n# Get feature names from the dataframe\nfeature_names = df.drop('Outcome', axis=1).columns\n\n# Function to plot feature importances\ndef plot_feature_importances(importances, model_name):\n    # Sort the feature importances in descending order and get the indices\n    sorted_indices = np.argsort(importances)[::-1]\n    plt.figure(figsize=(10, 6))\n    plt.title(f'Feature Importances in {model_name}', fontsize=16)\n    # Create the horizontal bar plot\n    plt.barh(range(len(sorted_indices)), importances[sorted_indices], color='skyblue', align='center')\n    # Set the y-ticks to be the feature names\n    plt.yticks(range(len(sorted_indices)), [feature_names[i] for i in sorted_indices], fontsize=12)\n    plt.xlabel('Relative Importance', fontsize=14)\n    plt.tight_layout()  # Adjusts plot to ensure everything fits without overlapping\n    plt.show()\n\n# Example usage with Random Forest feature importances\nif 'RandomForestClassifier' in models:\n    rf_importances = models['RandomForestClassifier'].feature_importances_\n    plot_feature_importances(rf_importances, 'Random Forest')\n    \n    \nif 'SVC' in models and hasattr(models['SVC'], 'coef_'):\n    svc_coefs = models['SVC'].coef_[0]\n    plot_feature_importances(svc_coefs, 'SVC')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:13:12.480150Z","iopub.execute_input":"2024-03-21T22:13:12.480528Z","iopub.status.idle":"2024-03-21T22:13:12.840769Z","shell.execute_reply.started":"2024-03-21T22:13:12.480500Z","shell.execute_reply":"2024-03-21T22:13:12.839183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# 1. Checking for Missing Values\nmissing_values = df.isnull().sum()\n\n# 2. Checking Data Types\ndata_types = df.dtypes\n\n# 3. Outlier Detection (Using a simple method like IQR for demonstration)\n# Here, just calculating IQR for each column but not removing outliers\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\noutliers = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()\n\n# 4. Checking for Duplicate Rows\nduplicate_rows = df.duplicated().sum()\n\nmissing_values, data_types, outliers, duplicate_rows\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:15:02.156742Z","iopub.execute_input":"2024-03-21T22:15:02.157133Z","iopub.status.idle":"2024-03-21T22:15:02.180670Z","shell.execute_reply.started":"2024-03-21T22:15:02.157103Z","shell.execute_reply":"2024-03-21T22:15:02.179608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nfor i, col in enumerate(df.columns[:-1]): # Exclude 'Outcome' column\n    plt.subplot(3, 3, i + 1)\n    sns.boxplot(y=df[col])\n    plt.title(col)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-19T14:02:22.056133Z","iopub.execute_input":"2024-03-19T14:02:22.056570Z","iopub.status.idle":"2024-03-19T14:02:23.671945Z","shell.execute_reply.started":"2024-03-19T14:02:22.056539Z","shell.execute_reply":"2024-03-19T14:02:23.670881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Calculating IQR for each column\nQ1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\n\n# Identifying rows with outliers\noutlier_indices = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)\noutlier_rows = df[outlier_indices]\n\nprint(outlier_rows)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:15:35.710897Z","iopub.execute_input":"2024-03-21T22:15:35.711232Z","iopub.status.idle":"2024-03-21T22:15:35.731992Z","shell.execute_reply.started":"2024-03-21T22:15:35.711206Z","shell.execute_reply":"2024-03-21T22:15:35.730894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Calculating IQR for 'BMI' and 'SkinThickness'\nQ1_BMI = df['BMI'].quantile(0.25)\nQ3_BMI = df['BMI'].quantile(0.75)\nIQR_BMI = Q3_BMI - Q1_BMI\n\nQ1_SkinThickness = df['SkinThickness'].quantile(0.25)\nQ3_SkinThickness = df['SkinThickness'].quantile(0.75)\nIQR_SkinThickness = Q3_SkinThickness - Q1_SkinThickness\n\n# Identifying rows with outliers in 'BMI' or 'SkinThickness'\noutliers_BMI = (df['BMI'] < (Q1_BMI - 1.5 * IQR_BMI)) | (df['BMI'] > (Q3_BMI + 1.5 * IQR_BMI))\noutliers_SkinThickness = (df['SkinThickness'] < (Q1_SkinThickness - 1.5 * IQR_SkinThickness)) | (df['SkinThickness'] > (Q3_SkinThickness + 1.5 * IQR_SkinThickness))\n\n# Filtering rows\noutlier_rows_bmi_skin = df[outliers_BMI | outliers_SkinThickness]\n\nprint(outlier_rows_bmi_skin)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:15:49.514218Z","iopub.execute_input":"2024-03-21T22:15:49.514584Z","iopub.status.idle":"2024-03-21T22:15:49.530647Z","shell.execute_reply.started":"2024-03-21T22:15:49.514556Z","shell.execute_reply":"2024-03-21T22:15:49.529908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Excluding 'Outcome' and 'Pregnancies' from the columns to check\ncolumns_to_check = df.columns[~df.columns.isin(['Outcome', 'Pregnancies'])]\n\n# Checking for zeros in the selected columns\nrows_with_zeros = df[columns_to_check].eq(0).any(axis=1)\n\n# Filtering rows that have at least one zero value in the selected columns\nrows_with_at_least_one_zero_corrected = df[rows_with_zeros]\n\nprint(rows_with_at_least_one_zero_corrected)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T06:19:00.540102Z","iopub.execute_input":"2024-03-21T06:19:00.540508Z","iopub.status.idle":"2024-03-21T06:19:00.556420Z","shell.execute_reply.started":"2024-03-21T06:19:00.540480Z","shell.execute_reply":"2024-03-21T06:19:00.555572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping the 'Insulin' and 'SkinThickness' columns\ndf_dropped = df.drop(columns=['Insulin', 'SkinThickness'])\n\nprint(df_dropped)\n#now i have two dfs to perform my test","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:16:26.956848Z","iopub.execute_input":"2024-03-21T22:16:26.958114Z","iopub.status.idle":"2024-03-21T22:16:26.969044Z","shell.execute_reply.started":"2024-03-21T22:16:26.958067Z","shell.execute_reply":"2024-03-21T22:16:26.967529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nQ1 = df_dropped.quantile(0.25)\nQ3 = df_dropped.quantile(0.75)\nIQR = Q3 - Q1\n\n# Capping the outliers\ndf_capped = df_dropped.clip(Q1 - 1.5 * IQR, Q3 + 1.5 * IQR, axis=1)\n\nprint(df_capped.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:16:29.705599Z","iopub.execute_input":"2024-03-21T22:16:29.706032Z","iopub.status.idle":"2024-03-21T22:16:29.729368Z","shell.execute_reply.started":"2024-03-21T22:16:29.706003Z","shell.execute_reply":"2024-03-21T22:16:29.728221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\nplt.figure(figsize=(15, 10))\nfor i, col in enumerate(df_capped.columns[:-1]): # Exclude 'Outcome' column\n    plt.subplot(3, 3, i + 1)\n    sns.boxplot(y=df_capped[col])\n    plt.title(col)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:16:33.302974Z","iopub.execute_input":"2024-03-21T22:16:33.303358Z","iopub.status.idle":"2024-03-21T22:16:34.258148Z","shell.execute_reply.started":"2024-03-21T22:16:33.303324Z","shell.execute_reply":"2024-03-21T22:16:34.256736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Applying Min-Max Scaling to the columns of df_capped except 'Outcome'\nfor col in df_capped.columns:\n    if col != 'Outcome':\n        df_capped[col] = (df_capped[col] - df_capped[col].min()) / (df_capped[col].max() - df_capped[col].min())\n\n# Displaying the first few rows of the normalized DataFrame\ndf_capped.head()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T12:33:20.621970Z","iopub.execute_input":"2024-03-21T12:33:20.622435Z","iopub.status.idle":"2024-03-21T12:33:20.654090Z","shell.execute_reply.started":"2024-03-21T12:33:20.622401Z","shell.execute_reply":"2024-03-21T12:33:20.653190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature Engeeniering\n# Creating interaction terms for the specified pairs of variables in df_capped\n\n# Interaction between DiabetesPedigreeFunction and Glucose\ndf_capped['DiabetesPedigreeFunction_Glucose_Interaction'] = df_capped['DiabetesPedigreeFunction'] * df_capped['Glucose']\n\n# Interaction between Age and Pregnancy\ndf_capped['Age_Pregnancy_Interaction'] = df_capped['Age'] * df_capped['Pregnancies']\n\n# Displaying the first few rows of the DataFrame with the new interaction terms\ndf_capped\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:16:43.357976Z","iopub.execute_input":"2024-03-21T22:16:43.359100Z","iopub.status.idle":"2024-03-21T22:16:43.386656Z","shell.execute_reply.started":"2024-03-21T22:16:43.359035Z","shell.execute_reply":"2024-03-21T22:16:43.385770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Binning the 'Age' column into three bins and 'BloodPressure' into two bins\n\n# Binning 'Age'\nage_bins = pd.cut(df_capped['Age'], bins=3, labels=[\"Young\", \"Middle-aged\", \"Senior\"])\n\n# Binning 'BloodPressure'\nblood_pressure_bins = pd.cut(df_capped['BloodPressure'], bins=2, labels=[\"Low\", \"High\"])\n\n# Adding the binned columns to the DataFrame\ndf_capped['Age_Binned'] = age_bins\ndf_capped['BloodPressure_Binned'] = blood_pressure_bins\n\n# Displaying the first few rows of the DataFrame with the new binned columns\ndf_capped_binning_head = df_capped.head()\ndf_capped_binning_head\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:17:02.640514Z","iopub.execute_input":"2024-03-21T22:17:02.640882Z","iopub.status.idle":"2024-03-21T22:17:02.666886Z","shell.execute_reply.started":"2024-03-21T22:17:02.640853Z","shell.execute_reply":"2024-03-21T22:17:02.665830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the skewness of the two features\nskewness_interaction_terms = df_capped[['DiabetesPedigreeFunction_Glucose_Interaction', 'Age_Pregnancy_Interaction']].skew()\nskewness_interaction_terms\n\n# Deciding on the transformation based on the type of skewness\n# For right-skewed data, we can use log transformation, but we need to check for non-positive values first.\n# Checking for non-positive values in the columns\nnon_positive_values_diabetes_glucose = (df_capped['DiabetesPedigreeFunction_Glucose_Interaction'] <= 0).any()\nnon_positive_values_age_pregnancy = (df_capped['Age_Pregnancy_Interaction'] <= 0).any()\n\nnon_positive_values_diabetes_glucose, non_positive_values_age_pregnancy\n\n# If there are no non-positive values, we can directly apply the log transformation\n# Otherwise, we'll add a constant before applying the transformation\nimport numpy as np\n\n# Adding a constant (1) and applying log transformation\ndf_capped['DiabetesPedigreeFunction_Glucose_Interaction_Log'] = np.log(df_capped['DiabetesPedigreeFunction_Glucose_Interaction'] + 1)\ndf_capped['Age_Pregnancy_Interaction_Log'] = np.log(df_capped['Age_Pregnancy_Interaction'] + 1)\ndf_capped.drop(['Age_Pregnancy_Interaction' ,'DiabetesPedigreeFunction_Glucose_Interaction' ] , axis=1 , inplace=True)\n\n# Checking the skewness after the transformation\nnew_skewness_interaction_terms = df_capped[['DiabetesPedigreeFunction_Glucose_Interaction_Log', 'Age_Pregnancy_Interaction_Log']].skew()\nnew_skewness_interaction_terms\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:17:09.381938Z","iopub.execute_input":"2024-03-21T22:17:09.382285Z","iopub.status.idle":"2024-03-21T22:17:09.403416Z","shell.execute_reply.started":"2024-03-21T22:17:09.382259Z","shell.execute_reply":"2024-03-21T22:17:09.402411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the distribution of all columns in df_capped including the newly transformed columns\n\nplt.figure(figsize=(20, 15))\n\nfor i, col in enumerate(df_capped.columns):\n    plt.subplot((len(df_capped.columns) + 2) // 3, 3, i+1)\n    if df_capped[col].dtype == 'object' or df_capped[col].nunique() <= 10:\n        # For categorical variables or those with a limited number of unique values, use a count plot\n        sns.countplot(x=col, data=df_capped)\n    else:\n        # For continuous variables, use a histogram\n        sns.histplot(df_capped[col], kde=True)\n    plt.title(f'Distribution of {col}')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T12:33:47.259735Z","iopub.execute_input":"2024-03-21T12:33:47.260206Z","iopub.status.idle":"2024-03-21T12:33:51.529825Z","shell.execute_reply.started":"2024-03-21T12:33:47.260171Z","shell.execute_reply":"2024-03-21T12:33:51.528467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\n# Selecting a few key variables for polynomial features\nselected_columns = ['Glucose', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n\n# Extracting the selected columns\nX = df_capped[selected_columns]\n\n# Creating polynomial features with degree 2\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Creating a DataFrame for the new polynomial features\npoly_feature_names = poly.get_feature_names_out(input_features=selected_columns)\ndf_poly = pd.DataFrame(X_poly, columns=poly_feature_names)\n\n# Showing the first few rows of the new DataFrame with polynomial features\ndf_poly_head = df_poly.head()\ndf_poly_head\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:17:43.436765Z","iopub.execute_input":"2024-03-21T22:17:43.437121Z","iopub.status.idle":"2024-03-21T22:17:43.464864Z","shell.execute_reply.started":"2024-03-21T22:17:43.437096Z","shell.execute_reply":"2024-03-21T22:17:43.463656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_poly['Outcome'] = df_capped['Outcome'].values\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:17:46.523426Z","iopub.execute_input":"2024-03-21T22:17:46.523824Z","iopub.status.idle":"2024-03-21T22:17:46.529575Z","shell.execute_reply.started":"2024-03-21T22:17:46.523792Z","shell.execute_reply":"2024-03-21T22:17:46.528540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\n\n\n\nX_df_poly = df_poly.drop(target_column, axis=1)\ny_df_poly = df_poly[target_column]\nX_train_df_poly, X_test_df_poly, y_train_df_poly, y_test_df_poly = train_test_split(X_df_poly, y_df_poly, test_size=0.2, random_state=42)\n\n\nfrom sklearn.metrics import accuracy_score\n\nmodel_performance_df_poly = {}\n\nfor model_name, model in models.items():\n    # Train the model on the training set\n    model.fit(X_train_df_poly, y_train_df_poly)\n    \n    # Predict on the test set\n    predictions = model.predict(X_test_df_poly)\n    \n    # Calculate the accuracy\n    accuracy = accuracy_score(y_test_df_poly, predictions)\n    \n    # Store the performance\n    model_performance_df_poly[model_name] = accuracy\n\n# Display the performance of each model on df_poly test set\nfor model_name, accuracy in model_performance_df_poly.items():\n    print(f\"The accuracy of {model_name} on df_poly test set is: {accuracy:.2f}\")\n\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:19:03.963891Z","iopub.execute_input":"2024-03-21T22:19:03.964236Z","iopub.status.idle":"2024-03-21T22:19:04.263256Z","shell.execute_reply.started":"2024-03-21T22:19:03.964212Z","shell.execute_reply":"2024-03-21T22:19:04.262044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\n\n# Define your DataFrame and target variable\n# Assuming you have already defined df_capped\n\n# Selecting features and target variable\nX_df_capped = df_capped.drop('Outcome', axis=1)\ny_df_capped = df_capped['Outcome']\n\n# Splitting the data into training and testing sets for df_capped\nX_train_df_capped, X_test_df_capped, y_train_df_capped, y_test_df_capped = train_test_split(X_df_capped, y_df_capped, test_size=0.2, random_state=42)\n\n# Define categorical and numeric features\ncategorical_features = ['Age_Binned', 'BloodPressure_Binned']  # Update with your categorical features\nnumeric_features = [col for col in X_df_capped.columns if col not in categorical_features]\n\n# Preprocessing: One-hot encode categorical features and scale numeric features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(), categorical_features)\n    ],\n    remainder='passthrough'\n)\n\n# Fit and transform the preprocessor on X_train_capped\nX_train_df_capped_preprocessed = preprocessor.fit_transform(X_train_df_capped)\n\n# Transform X_test_capped using the preprocessor fitted on X_train_capped\nX_test_df_capped_preprocessed = preprocessor.transform(X_test_df_capped)\n\n# Initialize the models\nlog_reg = LogisticRegression()\ndec_tree = DecisionTreeClassifier()\nrand_forest = RandomForestClassifier()\nsvm = SVC()\nknn = KNeighborsClassifier()\n\n# Dictionary to hold the models\nmodels = {\n    \"Logistic Regression\": log_reg,\n    \"Decision Tree\": dec_tree,\n    \"Random Forest\": rand_forest,\n    \"SVM\": svm,\n    \"KNN\": knn,\n    \n}\n\nprint(\"Model Training and Evaluation with Capped Features (df_capped):\")\n\n# Training and evaluating each model on df_capped\nfor name, model in models.items():\n    print(f\"\\nModel: {name}\")\n\n    # Training\n    model.fit(X_train_df_capped_preprocessed, y_train_df_capped)\n\n    # Predicting\n    y_pred_df_capped = model.predict(X_test_df_capped_preprocessed)\n\n    # Evaluating\n    accuracy_capped = accuracy_score(y_test_df_capped, y_pred_df_capped)\n    report_capped = classification_report(y_test_df_capped, y_pred_df_capped)\n    print(f\"Accuracy: {accuracy_capped}\")\n    print(\"Classification Report:\")\n    print(report_capped)\n\n    # Cross-validation\n    cross_val_acc_capped = cross_val_score(model, X_train_df_capped_preprocessed, y_train_df_capped, cv=5, scoring='accuracy').mean()\n    print(f\"Cross-Validation Accuracy: {cross_val_acc_capped}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:19:20.070609Z","iopub.execute_input":"2024-03-21T22:19:20.071037Z","iopub.status.idle":"2024-03-21T22:19:21.412482Z","shell.execute_reply.started":"2024-03-21T22:19:20.071006Z","shell.execute_reply":"2024-03-21T22:19:21.411483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\nif 'Outcome' not in df_poly.columns:\n    df_poly['Outcome'] = None  # Initialize the column if it doesn't exist\n\n# Identify non-numeric columns\nnon_numeric_columns = df_poly.select_dtypes(include=['object']).columns\n\n# Apply One-Hot Encoding if the data is nominal, or Label Encoding if it's ordinal\nfor col in non_numeric_columns:\n    if df_poly[col].nunique() < 10:  # assuming a low cardinality for One-Hot Encoding\n        df_poly = pd.get_dummies(df_poly, columns=[col])\n    else:\n        le = LabelEncoder()\n        df_poly[col] = le.fit_transform(df_poly[col])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:20:11.806099Z","iopub.execute_input":"2024-03-21T22:20:11.806441Z","iopub.status.idle":"2024-03-21T22:20:11.815024Z","shell.execute_reply.started":"2024-03-21T22:20:11.806414Z","shell.execute_reply":"2024-03-21T22:20:11.813586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_distributions = {\n    'RandomForestClassifier': {\n        'n_estimators': range(10, 200),\n        'max_depth': range(1, 20),\n        'min_samples_split': range(2, 20)\n    },\n    'LogisticRegression': {\n        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n        'solver': ['lbfgs', 'liblinear']\n    },\n    'SVC': {\n        'C': [0.1, 1, 10, 100],\n        'gamma': [1, 0.1, 0.01, 0.001],\n        'kernel': ['rbf', 'linear']\n    },\n    'DecisionTreeClassifier': {\n        'max_depth': range(1, 20),\n        'min_samples_split': range(2, 20),\n        'min_samples_leaf': range(1, 20)\n    },\n    'KNeighborsClassifier': {\n        'n_neighbors': range(1, 30),\n        'weights': ['uniform', 'distance'],\n        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n    }\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T22:20:16.293900Z","iopub.execute_input":"2024-03-21T22:20:16.294305Z","iopub.status.idle":"2024-03-21T22:20:16.301371Z","shell.execute_reply.started":"2024-03-21T22:20:16.294277Z","shell.execute_reply":"2024-03-21T22:20:16.299998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\nprint(\"Original columns:\", df_capped.columns)\nfrom sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\n\n# Define the categorical features you want to one-hot encode\ncategorical_features = ['Age_Binned', 'BloodPressure_Binned']\n\n# Initialize the OneHotEncoder\none_hot = OneHotEncoder()\n\n# Fit the OneHotEncoder and transform the categorical features\none_hot_encoded = one_hot.fit_transform(df_capped[categorical_features]).toarray()\n\n# Create a DataFrame with the one-hot encoded variables\none_hot_encoded_df = pd.DataFrame(one_hot_encoded, columns=one_hot.get_feature_names_out())\n\n# Drop the original categorical columns from df_capped\ndf_capped_dropped = df_capped.drop(categorical_features, axis=1)\n\n# Concatenate the one-hot encoded columns with the rest of the columns\ndf_capped_transformed = pd.concat([df_capped_dropped.reset_index(drop=True), one_hot_encoded_df], axis=1)\n\n\n# Define target column\ntarget_column = 'Outcome'\n\n# Split the dataset\nX_df_capped_transformed = df_capped_transformed.drop(target_column, axis=1)\ny_df_capped_transformed = df_capped_transformed[target_column]\nX_train_df_capped_transformed, X_test_df_capped_transformed, y_train_df_capped_transformed, y_test_df_capped_transformed = train_test_split(X_df_capped_transformed, y_df_capped_transformed, test_size=0.2, random_state=42)\n\n\n# Creating a dictionary of datasets\ndatasets = {\n    'df_capped_trasformed': (X_train_df_capped_transformed, y_train_df_capped_transformed, X_test_df_capped_transformed, y_test_df_capped_transformed)\n}\n\n\n# Define parameter distributions\n\n\ndef find_best_hyperparameters(datasets, models, param_distributions):\n    best_params = {dataset_name: {} for dataset_name in datasets}\n    \n    for dataset_name, (X_train, y_train, _, _) in datasets.items():\n        for model_name, model in models.items():\n            if model_name in param_distributions:  # If there are parameters to tune\n                random_search = RandomizedSearchCV(model, param_distributions[model_name], n_iter=20, cv=5, random_state=42)\n                random_search.fit(X_train, y_train)\n                best_params[dataset_name][model_name] = random_search.best_params_\n            else:  # For models without hyperparameters to tune, like GaussianNB\n                model.fit(X_train, y_train)\n                best_params[dataset_name][model_name] = \"No hyperparameters to tune\"\n    \n    return best_params\n\nbest_params_df_capped_transformed=find_best_hyperparameters(datasets, models, param_distributions)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:37:37.359092Z","iopub.execute_input":"2024-03-21T15:37:37.359584Z","iopub.status.idle":"2024-03-21T15:38:14.783919Z","shell.execute_reply.started":"2024-03-21T15:37:37.359527Z","shell.execute_reply":"2024-03-21T15:38:14.782811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndatasets_poly = {\n    'df_poly': (X_train_df_poly, y_train_df_poly, X_test_df_poly, y_test_df_poly)\n}\n\n\n\nbest_params_df_poly=find_best_hyperparameters(datasets_poly, models, param_distributions)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:38:14.789914Z","iopub.execute_input":"2024-03-21T15:38:14.793327Z","iopub.status.idle":"2024-03-21T15:38:53.991968Z","shell.execute_reply.started":"2024-03-21T15:38:14.793275Z","shell.execute_reply":"2024-03-21T15:38:53.990956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_poly_best_params = {\n    'RandomForestClassifier': {'n_estimators': 73, 'min_samples_split': 14, 'max_depth': 5},\n    'LogisticRegression': {'solver': 'lbfgs', 'C': 100},\n    'SVC': {'kernel': 'linear', 'gamma': 0.001, 'C': 1},\n    'DecisionTreeClassifier': {'min_samples_split': 12, 'min_samples_leaf': 15, 'max_depth': 17},\n    'GaussianNB': 'No hyperparameters to tune',\n    'KNeighborsClassifier': {'weights': 'distance', 'n_neighbors': 23, 'algorithm': 'brute'}\n}\n\n# Fit each model with its best parameters on df_poly\nfitted_models_df_poly = {}\n\nfor model_name, model in models.items():\n    # If the model has specific best parameters found, use them\n    if df_poly_best_params[model_name] != \"No hyperparameters to tune\":\n        model.set_params(**df_poly_best_params[model_name])\n    \n    # Fit the model\n    model.fit(X_train_df_poly, y_train_df_poly)  # Make sure to use the correct training set variables\n    \n    # Store the fitted model\n    fitted_models_df_poly[model_name] = model\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:21:54.828101Z","iopub.execute_input":"2024-03-21T15:21:54.828645Z","iopub.status.idle":"2024-03-21T15:21:55.125132Z","shell.execute_reply.started":"2024-03-21T15:21:54.828603Z","shell.execute_reply":"2024-03-21T15:21:55.123842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_capped_best_params = {\n    'RandomForestClassifier': {'n_estimators': 13, 'min_samples_split': 18, 'max_depth': 16},\n    'LogisticRegression': {'solver': 'lbfgs', 'C': 10},\n    'SVC': {'kernel': 'linear', 'gamma': 1, 'C': 10},\n    'DecisionTreeClassifier': {'min_samples_split': 17, 'min_samples_leaf': 19, 'max_depth': 7},\n    'GaussianNB': 'No hyperparameters to tune',\n    'KNeighborsClassifier': {'weights': 'distance', 'n_neighbors': 27, 'algorithm': 'brute'}\n}\n\n# Fit each model with its best parameters on df_capped\nfitted_models_df_capped = {}\n\nfor model_name, model in models.items():\n    # If the model has specific best parameters found, use them\n    if df_capped_best_params[model_name] != \"No hyperparameters to tune\":\n        model.set_params(**df_capped_best_params[model_name])\n    \n    # Fit the model\n    model.fit(X_train, y_train)\n    \n    # Store the fitted model\n    fitted_models_df_capped[model_name] = model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming best_hyperparameters is obtained from the previous step\n# df_capped_best_params = best_hyperparameters['df_capped']\n#best_params\ndf_capped_best_params = {\n    'RandomForestClassifier': {'n_estimators': 13, 'min_samples_split': 18, 'max_depth': 16},\n    'LogisticRegression': {'solver': 'lbfgs', 'C': 10},\n    'SVC': {'kernel': 'linear', 'gamma': 1, 'C': 10},\n    'DecisionTreeClassifier': {'min_samples_split': 17, 'min_samples_leaf': 19, 'max_depth': 7},\n    'GaussianNB': 'No hyperparameters to tune',\n    'KNeighborsClassifier': {'weights': 'distance', 'n_neighbors': 27, 'algorithm': 'brute'}\n}\n\n# Fit each model with its best parameters on df_capped\nfitted_models_df_capped = {}\n\nfor model_name, model in models.items():\n    # If the model has specific best parameters found, use them\n    if df_capped_best_params[model_name] != \"No hyperparameters to tune\":\n        model.set_params(**df_capped_best_params[model_name])\n    \n    # Fit the model\n    model.fit(X_train, y_train)\n    \n    # Store the fitted model\n    fitted_models_df_capped[model_name] = model\n    \n    \n\n# Now, fitted_models_df_capped contains all the models fitted with their best parameters\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T14:33:07.617002Z","iopub.execute_input":"2024-03-21T14:33:07.618090Z","iopub.status.idle":"2024-03-21T14:33:07.723501Z","shell.execute_reply.started":"2024-03-21T14:33:07.618032Z","shell.execute_reply":"2024-03-21T14:33:07.722009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate each fitted model on the test set of df_capped\nmodel_performance_df_poly = {}\n\nfor model_name, model in fitted_models_df_poly.items():\n    # Predict using the fitted model\n    predictions = model.predict(X_test_df_poly)\n    \n    # Calculate the accuracy or any other performance metric\n    accuracy = (predictions == y_test_df_poly).mean()\n    \n    # Store the performance\n    model_performance_df_poly[model_name] = accuracy\n\n# Display the performance of each model\nfor model_name, accuracy in model_performance_df_poly.items():\n    print(f\"The accuracy of {model_name} on df_poly test set is: {accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:24:30.629395Z","iopub.execute_input":"2024-03-21T15:24:30.630116Z","iopub.status.idle":"2024-03-21T15:24:30.820993Z","shell.execute_reply.started":"2024-03-21T15:24:30.630061Z","shell.execute_reply":"2024-03-21T15:24:30.819914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate each fitted model on the test set of df_capped\nmodel_performance_df_capped_transformed = {}\n\nfor model_name, model in fitted_models_df_capped.items():\n    # Predict using the fitted model\n    predictions = model.predict(X_test_df_capped_transformed)\n    \n    # Calculate the accuracy or any other performance metric\n    accuracy = (predictions == y_test_df_capped_transformed).mean()\n    \n    # Store the performance\n    model_performance_df_capped_transformed[model_name] = accuracy\n\n# Display the performance of each model\nfor model_name, accuracy in model_performance_df_capped_transformed.items():\n    print(f\"The accuracy of {model_name} on df_capped test set is: {accuracy:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T14:55:47.085628Z","iopub.execute_input":"2024-03-21T14:55:47.086103Z","iopub.status.idle":"2024-03-21T14:55:47.126102Z","shell.execute_reply.started":"2024-03-21T14:55:47.086071Z","shell.execute_reply":"2024-03-21T14:55:47.125147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_poly['Outcome'] = df_capped['Outcome'].values\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T13:09:39.651376Z","iopub.execute_input":"2024-03-20T13:09:39.652410Z","iopub.status.idle":"2024-03-20T13:09:39.658235Z","shell.execute_reply.started":"2024-03-20T13:09:39.652374Z","shell.execute_reply":"2024-03-20T13:09:39.657160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"models","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming model_performance_df_capped_transformed, model_performance_df_poly, and model_performance_df\n# are dictionaries with model names as keys and their respective accuracies as values\n\ncombined_accuracies = {\n    'df_capped_transformed': model_performance_df_capped_transformed,\n    'df_poly': model_performance_df_poly,\n    'df': model_performance_df\n}\n\n# Create a DataFrame from the combined accuracies\ndf_accuracies = pd.DataFrame(combined_accuracies)\n\n# Display the table of accuracies\nprint(\"Model Accuracies on Different Datasets:\")\nprint(df_accuracies)\n\n# Plot the accuracies\nax = df_accuracies.plot(kind='bar', figsize=(10, 6))\nax.set_title('Model Accuracies on Different Datasets')\nax.set_xlabel('Models')\nax.set_ylabel('Accuracy')\nplt.xticks(rotation=45)\nplt.legend(title='Dataset')\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T15:28:34.676431Z","iopub.execute_input":"2024-03-21T15:28:34.677051Z","iopub.status.idle":"2024-03-21T15:28:35.233811Z","shell.execute_reply.started":"2024-03-21T15:28:34.677012Z","shell.execute_reply":"2024-03-21T15:28:35.232370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}